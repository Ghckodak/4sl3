{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = pd.read_table('Data/data_banknote_authentication.txt', sep = \",\", header=None)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "t = dataset.iloc[:, -1].values\n",
    "\n",
    "# split the training set, validation set and test set of ratio 6:2:2\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=8775)\n",
    "X_train, X_valid, t_train, t_valid = train_test_split(X_train, t_train, test_size=0.25, random_state=8775)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "X_valid = sc.transform(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "def standard_normal(matrix):    # mean=0, stdev=1\n",
    "    return np.random.standard_normal(size=matrix.shape)\n",
    "\n",
    "def random_integers(matrix):    # random 1 -1\n",
    "    opt = np.random.choice([0,1],size=matrix.shape)\n",
    "    return np.where(opt==0, -1, opt)\n",
    "\n",
    "def pattern(matrix):    # pattern matrix of 1 -1 1 -1 1 -1\n",
    "    for m in range (matrix.shape[0]):\n",
    "\n",
    "        if np.matrix.ndim != 1:\n",
    "            for n in range (matrix.shape[1]):\n",
    "                matrix[m][n] = (m+n)%2\n",
    "        else:\n",
    "            matrix[m] = m%2\n",
    "\n",
    "    return np.where(matrix==0, -1, matrix)\n",
    "\n",
    "def cross_entropy(y,t):\n",
    "    eps = np.finfo(float).eps\n",
    "    cross_entropy = -np.sum(t * np.log(y + eps))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def NNClassifier(X, t, hidden_layer_sizes, initilize, epochs, learning_rate):\n",
    "    layer_1_size = hidden_layer_sizes[0]\n",
    "    layer_2_size = hidden_layer_sizes[1]\n",
    "    \n",
    "    output = np.zeros(len(X))\n",
    "\n",
    "    # initilize the matrix for best weights\n",
    "    w_1_best = np.ones((layer_1_size, 5))\n",
    "    w_2_best = np.ones((layer_2_size, layer_1_size+1))\n",
    "    w_3_best = np.ones((1, layer_2_size+1))\n",
    "\n",
    "    # initilize weights for training\n",
    "    w_1 = initilize(w_1_best)\n",
    "    w_2 = initilize(w_2_best)\n",
    "    w_3 = initilize(w_3_best)\n",
    "\n",
    "    j=0\n",
    "    loss = np.ones(epochs)*np.inf\n",
    "\n",
    "    while(j<epochs):\n",
    "\n",
    "        X,t = shuffle(X,t)      # shuffle the set\n",
    "\n",
    "        for i in range (len(X)):    # loop through all examples \n",
    "            input = X[i]\n",
    "\n",
    "            # forward pass\n",
    "            z_1 = np.dot(w_1, np.insert(input,0,1).T)\n",
    "            h_1 = ReLU(z_1)\n",
    "\n",
    "            z_2 = np.dot(w_2, np.insert(h_1,0,1).T)\n",
    "            h_2 = ReLU(z_2)\n",
    "\n",
    "            z_3 = np.dot(w_3, np.insert(h_2,0,1).T)\n",
    "\n",
    "            output[i] = np.rint( np.power((1 + np.exp(-z_3)), -1) )\n",
    "\n",
    "            # backward pass\n",
    "            dz_3 = -output[i]+ np.power((1 + np.exp(-z_3)), -1)\n",
    "            gw_3 = dz_3*np.insert(h_2.T,0,1)\n",
    "            gz_2 = np.multiply(d_ReLU(z_2),np.dot( np.delete(w_3,0,1).T, dz_3))\n",
    "\n",
    "            gw_2 = gz_2.reshape(layer_2_size,1)*np.insert(h_1.T,0,1)\n",
    "            gz_1 = np.multiply(d_ReLU(z_1),np.dot( np.delete(w_2,0,1).T, gz_2))\n",
    "\n",
    "            gw_1 = gz_1.reshape(layer_1_size,1)*np.insert(input.T,0,1)\n",
    "\n",
    "            # update w\n",
    "            w_3 = w_3-learning_rate*gw_3\n",
    "            w_2 = w_2-learning_rate*gw_2\n",
    "            w_1 = w_1-learning_rate*gw_1\n",
    "\n",
    "        # calcuate loss of these particular group of weights\n",
    "        loss[j] = cross_entropy(output,t)   \n",
    "\n",
    "        if np.argmin(loss)==j:  # if the current result is minimum, update best weights\n",
    "            w_1_best = w_1\n",
    "            w_2_best = w_2\n",
    "            w_3_best = w_3\n",
    "\n",
    "\n",
    "        j=j+1\n",
    "\n",
    "    \n",
    "\n",
    "    return output, np.argmin(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "o, l=NNClassifier(X_train, t_train,hidden_layer_sizes=(3,5), initilize = random_integers, epochs=50,learning_rate=0.005)\n",
    "print(l)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
