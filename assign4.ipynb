{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "dataset = pd.read_csv('Data/spambase.data', header=None)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "t = dataset.iloc[:, -1].values\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size = 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classifier\n",
    "def tree_clf(iteration):\n",
    "    err = np.zeros(iteration)\n",
    "    for i in range (iteration):\n",
    "        tree_clf = DecisionTreeClassifier(criterion='entropy',splitter='random', max_leaf_nodes= i+2)\n",
    "        tree_clf.fit(X_train, t_train)\n",
    "        err[i] = -np.mean(np.asarray(cross_val_score(tree_clf, X, t, cv=5, scoring='neg_mean_squared_error')))\n",
    "    return err\n",
    "\n",
    "# Bagging classifier\n",
    "def bag_clf(iteration):\n",
    "\terr = np.zeros(iteration)\n",
    "\tfor i in range (iteration):\n",
    "\t\tbag_clf=BaggingClassifier(DecisionTreeClassifier(),n_jobs=-1, n_estimators=(i+1)*50)\t# loop from 50 to 2500\n",
    "\t\tbag_clf.fit(X_train, t_train)\n",
    "\t\terr[i] = -np.mean(np.asarray(cross_val_score(bag_clf, X, t, cv=5,scoring='neg_mean_squared_error')))\n",
    "\t\n",
    "\treturn err\n",
    "\n",
    "# Random forest classifier\n",
    "def rf_clf(iteration):\n",
    "    err = np.zeros(iteration)\n",
    "    for i in range (iteration):\n",
    "        rf_clf = RandomForestClassifier(n_jobs=-1, n_estimators=(i+1)*50)\n",
    "        rf_clf.fit(X_train, t_train)\n",
    "        err[i] = -np.mean(np.asarray(cross_val_score(rf_clf, X, t, cv=5,scoring='neg_mean_squared_error')))\n",
    "    return err\n",
    "\n",
    "# Adaboost classifiers with decision stumps\n",
    "def ab_clf(iteration):\n",
    "    err = np.zeros(iteration)\n",
    "    for i in range (iteration):\n",
    "        ab_clf = AdaBoostClassifier(n_estimators=(i+1)*50)\n",
    "        ab_clf.fit(X_train, t_train)\n",
    "        err[i] = -np.mean(np.asarray(cross_val_score(ab_clf, X, t, cv=5,scoring='neg_mean_squared_error')))\n",
    "    return err\n",
    "\n",
    "# Adaboost classifiers with decision trees with maximum 10 leaves\n",
    "def ab_clf_tree(iteration):\n",
    "    err = np.zeros(iteration)\n",
    "    for i in range (iteration):\n",
    "        ab_clf = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=10), n_estimators=(i+1)*50)\n",
    "        ab_clf.fit(X_train, t_train)\n",
    "        err[i] = -np.mean(np.asarray(cross_val_score(ab_clf, X, t, cv=5,scoring='neg_mean_squared_error')))\n",
    "    return err\n",
    "\n",
    "# Adaboost classifiers with decision trees with no restriction\n",
    "def ab_clf_tree_free(iteration):\n",
    "    err = np.zeros(iteration)\n",
    "    for i in range (iteration):\n",
    "        ab_clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=(i+1)*50)\n",
    "        ab_clf.fit(X_train, t_train)\n",
    "        err[i] = -np.mean(np.asarray(cross_val_score(ab_clf, X, t, cv=5,scoring='neg_mean_squared_error')))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_err = tree_clf(500)\n",
    "bag_err = bag_clf(50)\n",
    "rf_err = rf_clf(50)\n",
    "ab_err = ab_clf(50)\n",
    "ab_err_tree = ab_clf_tree(50)\n",
    "ab_err_tree_free = ab_clf_tree_free(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (np.arange(50)+1)*50 \n",
    "figure(figsize=(10, 6), dpi=100)\n",
    "\n",
    "plt.plot(tree_err)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x,np.ones(50)*np.min(tree_err),label='Decision tree')\n",
    "plt.plot(x,bag_err,label='Bagging')\n",
    "plt.plot(x,rf_err,label='Ramdom Forest')\n",
    "plt.plot(x,ab_err,label='Ab with decision stumps')\n",
    "plt.plot(x,ab_err_tree,label='Ab with trees of max 10 leaves')\n",
    "plt.plot(x,ab_err_tree_free,label='Ab with trees of no restriction',color='y')\n",
    "plt.xlabel(\"Number of predictors\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE of 5 ensemble methods vs Number of predictors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
